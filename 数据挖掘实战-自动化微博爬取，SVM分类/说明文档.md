该项目为微博爬虫以及数据分析说明文档

一共为5个代码组成:

![image-20231007141218985](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/202310071412417.png)

先来说说爬虫部分

爬虫部分分为spider_URL和spider_Data

spider_URL就是去爬取URL的，就是通过输入相关的热词，然后去爬取实时的用户对应的url

![image-20231007141501731](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/202310071415706.png)

接着通过spider_Data去模拟登陆然后去爬取相关的用户信息数据

![image-20231007141555404](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/202310071415830.png)

接着我们爬取相关信息之后，我们开始数据清洗工作以及数据建模工作

也就是train，我们先把相关的数据特征收集下来的，无用的则删除掉，这里根据提供的数据集进行训练

其中，我们把1作为真实用户，0则作为机器人

```Python
df1 = pd.read_excel('./train/train.xlsx',sheet_name='正常数据')
df2 = pd.read_excel('./train/train.xlsx',sheet_name='简介为空或关粉比大于3')

df1 = df1.drop(['uid','性别','ip属地'],axis=1)
df2 = df2.drop(['uid','性别','ip属地'],axis=1)

#1代表真实用户，0代表机器人
df1['用户特征'] = 1
df2['用户特征'] = 0

```

接着我们再去做数据清洗工作，把数字的归为数字，文字的进行筛选，去掉无用词以及机械压缩

```Python
def ip_data(x):
    x1 = str(x).split(" ")
    return x1[0]


def ip_chuli(x):
    x1 = str(x).split("：")
    return x1[-1]


def number_data(x):
    try:
        x1 = str(x)
        number = re.findall(r'\d+', x1)
        return number[0]
    except:
        return 0


def fan_chuli(x):
    try:
        x1 = int(x)
        return x1
    except:
        if '万' in x:
            x1 = str(x).replace('万','')
            x1 = float(x1) * 10000
            return x1


stop_words = []
with open("./data/stopwords_cn.txt", 'r', encoding='utf-8') as f:
    lines = f.readlines()
    for line in lines:
        stop_words.append(line.strip())


#去掉标点符号，以及机械压缩
def preprocess_word(word):
    word1 = str(word)
    word1 = re.sub(r'转发微博', '', word1)
    word1 = re.sub(r'#\w+#', '', word1)
    word1 = re.sub(r'【.*?】', '', word1)
    word1 = re.sub(r'@[\w]+', '', word1)
    word1 = re.sub(r'[a-zA-Z]', '', word1)
    word1 = re.sub(r'\.\d+', '', word1)
    return word1


def emjio_tihuan(x):
    x1 = str(x)
    x2 = re.sub('(\[.*?\])', "", x1)
    x3 = re.sub(r'@[\w\u2E80-\u9FFF]+:?|\[\w+\]', '', x2)
    x4 = re.sub(r'\n', '', x3)
    return x4


def is_all_chinese(strs):
    for _char in strs:
        if not '\u4e00' <= _char <= '\u9fa5':
            return False
    return True


# 定义机械压缩函数
def yasuo(st):
    for i in range(1, int(len(st) / 2) + 1):
        for j in range(len(st)):
            if st[j:j + i] == st[j + i:j + 2 * i]:
                k = j + i
                while st[k:k + i] == st[k + i:k + 2 * i] and k < len(st):
                    k = k + i
                st = st[:j] + st[k:]
    return st


def get_cut_words(content_series):
    # 读入停用词表
    # 分词
    word_num = jieba.lcut(content_series, cut_all=False)

    # 条件筛选
    word_num_selected = [i for i in word_num if i not in stop_words and len(i) >= 2 and is_all_chinese(i) == True]

    return ' '.join(word_num_selected)


def null_paichu(x):
    x1 = str(x)
    if len(x1) != 0:
        return x1
    else:
        return np.NAN


data['简介'] = data['简介'].apply(preprocess_word)
data['简介'] = data['简介'].apply(emjio_tihuan)
data['简介'] = data['简介'].apply(yasuo)
data['简介'] = data['简介'].apply(get_cut_words)
data['简介'] = data['简介'].apply(null_paichu)
data['昵称'] = data['昵称'].apply(preprocess_word)
data['昵称'] = data['昵称'].apply(emjio_tihuan)
data['昵称'] = data['昵称'].apply(yasuo)
data['昵称'] = data['昵称'].apply(get_cut_words)
data['昵称'] = data['昵称'].apply(null_paichu)
data = data.dropna(subset=['简介','昵称'],axis=0)
# 选取标签列
y = data['用户特征']
```



最后进行建模，以及特征选择，和数据归一化，使得我们的准确率得以提升

```Python
vectorizer = TfidfVectorizer()
#该类会统计每个词语的tf-idf权值
word1 = vectorizer.fit_transform(list(data['昵称']))
word2 = vectorizer.transform(list(data['简介']))
data1 = data[['粉丝量', '关注量', '博文数']]
# 创建归一化的实例
scaler = MinMaxScaler()
# 在数据集上进行归一化训练
scaler.fit(data1)
# 应用归一化转换
normalized_data = scaler.transform(data1)
x_num_features_sparse = scipy.sparse.csr_matrix(normalized_data)
#汇总所有特征
X = hstack([word1, word2,x_num_features_sparse])

# 分割训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(normalized_data, y, test_size=0.2, random_state=42)

# 训练SVM模型
clf = svm.SVC()
clf.fit(X_train, y_train)

# 进行预测
y_pred = clf.predict(X_test)

#创建分类报告
clf_report = classification_report(y_test, y_pred)

# 将分类报告保存为 .txt 文件
with open('./data/classification_report.txt', 'w') as f:
    f.write(clf_report)

dump(clf, './model/svm_model.joblib')
dump(scaler, './model/scaler_model.joblib')
```

训练好的模型放在model文件里面

![image-20231007142216672](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/202310071422907.png)

对应的相关指标则用记事本的方式记录下来：

![image-20231007142248698](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/202310071422152.png)

![image-20231007142303430](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/202310071423633.png)

这些相关指标是机器学习常见的相关指标，这里就不过多介绍，稍微去网上搜索 机器学习 xxx 就有相关的指标介绍了，里面写的更为详细

接着我们采用test对我们采集下来的data文件进行SVM分类预测，生成相关文件，对该文件进行数据可视化

![image-20231007142513650](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/202310071425787.png)

使用我们的data_analysis文件，生成6个可视化

![image-20231007142610613](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/202310071426934.png)

我们依次来介绍这些可视化的含义：

该图是代表真实用户以及机器人，他们的粉丝量、关注量、博文量的平均数，是统计该类型的全部数值，然后求的平均数

![image-20231007142656658](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/202310071426599.png)

![image-20231007142836540](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/202310071428290.png)

相关词云这个就没啥好解释，只是名字要区分一下

```
basic_wordcloud1-真实用户-昵称分析
basic_wordcloud2-机器人-昵称分析
basic_wordcloud3-真实用户-简介分析
basic_wordcloud4-机器人-简介分析
```

![image-20231007143121261](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/202310071431505.png)

中国地图，这里就是真实用户和机器人，他们归属地的分布数量，地图，根据左下角的颜色，可以区分数量的大小，数量越大，则对应的颜色也会发生变化

以上便是整个项目的解析，如果有什么不懂的，可以在群里讨论