# 网易分析项目

该项目一共有4个步骤：

1. 高频词、词云图
2. 情感打分
3. 地区分布情况
4. 聚类说明



# 高频词、词云图

该项目主要的库为：jieba、stylecloud



首先是读取数据，选择评论列作为数据源

对数据源进行去重



```Python
df = pd.read_excel('demo-最终版.xlsx')
df['评论'] = df['评论'].drop_duplicates(keep='first')
df = df.dropna(subset=['评论'], axis=0)
df['评论'] = df['评论'].astype(str)
```

然后进行中文判断，和停用词去除

把一些无效内容给全部去掉

主要核心是这两句

```Python
 # 分词
 word_num = jieba.lcut(content_series.str.cat(sep='。'), cut_all=False)

 # 条件筛选
 word_num_selected = [i for i in word_num if i not in stop_words and len(i) >= 2 and is_all_chinese(i) == True]
```



接着处理好之后就是采用如下代码，进行词云可视化

```Python 
text = get_cut_words(content_series=df['评论'])
    stylecloud.gen_stylecloud(text=' '.join(text), max_words=200,
                              collocations=False,
                              font_path='simhei.ttf',
                              icon_name='fas fa-circle',
                              size=500,
                              # palette='matplotlib.Inferno_9',
                              output_name='评论-词云图.png')
    Image(filename='评论-词云图.png')
```

剩下的就是在上面的基础上，筛选前200个高频词保存为csv文件



# 情感打分

该项目主要的库为：snownlp算法

该算法是目前国内最流行的情感分析库

其核心代码为：

```python 
if len(word_num_selected) != 0:
	score = SnowNLP(' '.join(word_num_selected))
	fenshu = score.sentiments
	return fenshu
else:
	return None
```

就是判断清洗好的数据是否为0，如果为0那么就返回空值，如果不为0，那么去计算文本的情感分数



分数的规则为，越接近1则为正向，越接近0则为负向

![情感分析](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90.jpg)

后面把数据进行可视化，会发现，大部分区域中立或者正向，也就是非负情感占主导



# 地区分布情况

地区分布则是采集数据里面的地区列

其主要的库为：pyecharts



先把数据进行清洗，把每个省或者市出现的次数统计好，然后再用一个列表进行封装

```Python 
def main1(x):
    x1 = str(x).split('-')
    x1 = x1[0].replace('省','').replace('市','')
    x1 = x1.strip(' ')
    return x1


df['所在地区'] = df['所在地区'].apply(main1)
new_df = df['所在地区'].value_counts()

list1 = []
for i in zip(new_df.index,new_df.values):
    list1.append([i[0],int(i[1])])
```

封装好之后，直接去调用函数，最后把地图呈现出来,这里颜色越深，说明出现的频次越高

![image-20221216171940802](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20221216171940802.png)



# 聚类说明

其核心库为：TfidfTransformer、MiniBatchKMeans

首先是先对词进行权重计算，然后进行统计文字里面的特征向量

根据这些的特征向量，然后对数据进行聚类处理

自动寻找一个中心点，相近的文本就会聚在一起，形成一个小团队，这里因为数据集较大的原因，因此采为3类

其核心代码为：

```
# 文档预料 空格连接
corpus = []
# 读取预料 一行预料为一个文档
for line in open('C-class-fenci.txt', 'r',encoding='utf-8').readlines():
    corpus.append(line.strip())
# 将文本中的词语转换为词频矩阵 矩阵元素a[i][j] 表示j词在i类文本下的词频
vectorizer = CountVectorizer()
# 该类会统计每个词语的tf-idf权值
transformer = TfidfTransformer()
# 第一个fit_transform是计算tf-idf 第二个fit_transform是将文本转为词频矩阵
tfidf = transformer.fit_transform(vectorizer.fit_transform(corpus))
# 获取词袋模型中的所有词语
word = vectorizer.get_feature_names()
# 将tf-idf矩阵抽取出来 元素w[i][j]表示j词在i类文本中的tf-idf权重
weight = tfidf.toarray()
```

```
clf = MiniBatchKMeans(n_clusters=n_clusters)
pre = clf.fit_predict(weight)
print(pre)

result = pd.concat((df, pd.DataFrame(pre)), axis=1)
result.rename({0: '聚类结果'}, axis=1, inplace=True)
result.to_csv('聚类结果.csv',encoding="utf-8-sig")
```

![image-20221216172616446](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20221216172616446.png)