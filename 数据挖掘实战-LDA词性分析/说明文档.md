# 该项目一共分四步走

## 1.首先进行数据处理

数据处理，我们用到的是pandas常用库

先进行去重drop_duplicates，把重复的内容处理好之后，我们开始删除一些无效的内容，例如表情包，无效词等

这里首先就是先去掉表情包，然后再判断该文本是否为中文，接着再去用停用词文本，去除无效词

![image-20230317115641588](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20230317115641588.png)



接着我们stylecloud进行词云图，这样方便我们看看整体的分词效果如何，是否有一些词要不要去掉

最后的结果如下：

![词云图1](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/%E8%AF%8D%E4%BA%91%E5%9B%BE1.png)

把结果都处理好了就开始NLP，情感分析

## 2、情感分析

文本挖掘中，情感分析是经常需要使用到，而进行主题模型分析之前，对数据集进行文本分类再进行分析具有必要性，因为分类以后，每一类的主题才会更明显。而snownlp是一个python写的类库，可以方便的处理中文文本内容

 这里我们还是根据上面的方法，先进行分词处理，因为这样更有效帮助机器进行文本判断，从而给出正确的评价，这里有一点是要注意的

就是snownlp毕竟是广义的，无法做到精准判断该文本的正确评分，只能给出大概的数值，这也是所有机器学习的通病，毕竟是机器，NLP还是主观占比居多，所以提供的数值仅供参考

![image-20230317115851894](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20230317115851894.png)



接着我们获取到对应的分值之后，我们可以根据数据来做一个时间趋势图，从而得知，在每个月的一个分值走向，这里才去的是均值处理，把每个月的所有分值相加求平均值，所以这里还是有一定的参考价值，可以作为正确的评判标准，这里分值是从0-1直接的，接近0则是负面，接近1则是正面

![情感趋势1](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/%E6%83%85%E6%84%9F%E8%B6%8B%E5%8A%BF1.png)

## 3、LDA主题建模

LDA参考文献：https://zhuanlan.zhihu.com/p/75222819

​			https://zhuanlan.zhihu.com/p/76636216

这里我们的步骤和上面和一样

首先我们还是先分词，把无效词给除去掉

接着我们开始构造主题数，寻找最优主题数，这里采用困惑度严格来说，判断标准并不合适，基于此我们这里采用的是另一种方式，也就是通过各个主题间的余弦相似度来衡量主题间的相似程度

![image-20230204223928349](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/202302042327880.png)

![image-20230204223941397](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/202302042327890.png)



具体代码实现方式

```python
# 构造主题数寻优函数
    def cos(vector1, vector2):  # 余弦相似度函数
        dot_product = 0.0
        normA = 0.0
        normB = 0.0
        for a, b in zip(vector1, vector2):
            dot_product += a * b
            normA += a ** 2
            normB += b ** 2
        if normA == 0.0 or normB == 0.0:
            return (None)
        else:
            return (dot_product / ((normA * normB) ** 0.5))

        # 主题数寻优

    def lda_k(x_corpus, x_dict):
        # 初始化平均余弦相似度
        mean_similarity = []
        mean_similarity.append(1)

        # 循环生成主题并计算主题间相似度
        for i in np.arange(2, 11):
            lda = models.LdaModel(x_corpus, num_topics=i, id2word=x_dict)  # LDA模型训练
            for j in np.arange(i):
                term = lda.show_topics(num_words=30)

            # 提取各主题词
            top_word = []
            for k in np.arange(i):

                top_word.append([''.join(re.findall('"(.*)"', i)) \
                                 for i in term[k][1].split('+')])  # 列出所有词

            # 构造词频向量
            word = sum(top_word, [])  # 列出所有的词
            unique_word = set(word)  # 去除重复的词

            # 构造主题词列表，行表示主题号，列表示各主题词
            mat = []
            for j in np.arange(i):
                top_w = top_word[j]
                mat.append(tuple([top_w.count(k) for k in unique_word]))

            p = list(itertools.permutations(list(np.arange(i)), 2))
            l = len(p)
            top_similarity = [0]
            for w in np.arange(l):
                vector1 = mat[p[w][0]]
                vector2 = mat[p[w][1]]
                top_similarity.append(cos(vector1, vector2))

            # 计算平均余弦相似度
            mean_similarity.append(sum(top_similarity) / l)
        return (mean_similarity)
```

处理好之后，再通过matplotlib来进行作图，在这里有3个低谷，可以选择4或者6或者8，根据最后lda呈现的模型去判断，效果好与坏，这样才用4

![主题数寻优](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/%E4%B8%BB%E9%A2%98%E6%95%B0%E5%AF%BB%E4%BC%982.png)

从而进行建模，最后呈现的效果图如下：

可以通过点击不同的圆圈，来查看不同主题下，不同主题词的权重，这里可以看出来选择4的效果挺不错的，圈圈分的比较开，不会重叠在一起

![image-20230317120153308](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-202303171201533082.png) 