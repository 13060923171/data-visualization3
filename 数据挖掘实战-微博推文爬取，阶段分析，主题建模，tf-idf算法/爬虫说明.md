# 爬虫流程说明

目标网站微博:https://weibo.com/u/5319384503/home?wvr=5



![image-20230628105552775](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20230628105552775.png)

微博是需要先登录才能去获取数据的，登录之后，打开搜索列表



![image-20230628105706061](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20230628105706061.png)

选择对应的时间信息，然后按F12，出现此页面

![image-20230628105809755](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20230628105809755.png)

然后刷新一下页面，找到第一个

![image-20230628105858781](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20230628105858781.png)

![image-20230628105927414](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20230628105927414.png)

这样我们就获取对应的API

![image-20230628105952901](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20230628105952901.png)

以及下面的COOKIE值

我们根据这里获取的API去构建，每一天对应的API,构建代码如下：

![image-20230628110056505](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20230628110056505.png)

通过这样我们就获取每一个，50页的内容，因为微博，最多只能呈现50页而已，所以我们就也构建每一天的50页，来获取全部时间的内容，在获取整个页面信息后，我们还需定位来获取每个对应的信息

![image-20230628110312461](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20230628110312461.png)

通过移动鼠标箭头，我们可以定位到对应div值

![image-20230628110359749](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20230628110359749.png)

然后来获取全部信息内容：

![image-20230628110428883](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20230628110428883.png)

这里采用的是XPATH语法定位规则来获取内容：

具体实现代码如下：

```Python
soup = etree.HTML(content)
node = soup.xpath('//div[@class="card-feed"]')
act = soup.xpath('//div[@class="card-act"]/ul')
for n,a in zip(node,act):
        try:
            name = n.xpath('./div[@node-type="like"]/div/div[2]/a/@nick-name')[0]

        except:
            name = np.NaN
        # try:
        #     title = n.xpath('./div[@class="avator"]/a/span/@title')[0]
        #
        # except:
        #     title = np.NaN
        try:
            timedate = n.xpath('./div[@node-type="like"]/div[2]/a/text()')[0]
        except:
            timedate = np.NaN
        try:
            comtent = n.xpath('./div[@node-type="like"]/p[@node-type="feed_list_content"]/text()')
            comtent1 = ' '.join(comtent)
        except:
            comtent1 = np.NaN
        try:
            dianzan = a.xpath('./li[3]/a/button/span[2]/text()')[0]

        except:
            dianzan =  np.NaN
        try:
            zhuanfa = a.xpath('./li[1]/a/text()')[0]

        except:
            zhuanfa =  np.NaN
        try:
            pinglun = a.xpath('./li[2]/a/text()')[0]

        except:
            pinglun =  np.NaN
        df = pd.DataFrame()
        df['时间'] = [timedate]
        df['博主'] = [name]
        df['内容'] = [comtent1]
        df['点赞'] = [dianzan]
        df['转发'] = [zhuanfa]
        df['评论'] = [pinglun]
        df.to_csv('钟薛高.csv', index=None, header=None, mode='a+', encoding='utf-8-sig')
    time.sleep(0.2)
```

最后的输出结果如下：

![image-20230628110609497](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20230628110609497.png)

![image-20230628110625729](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20230628110625729.png)

一共是2万多条数据内容