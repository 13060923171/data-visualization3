# 该项目一共分四步走

## 1.首先进行数据处理

数据处理，我们用到的是pandas常用库

先进行去重drop_duplicates，把重复的内容处理好之后，我们开始删除一些无效的内容，例如表情包，无效词等

这里首先就是先去掉表情包，然后再判断该文本是否为中文，接着再去用停用词文本，去除无效词

![image-20230207155608993](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20230207155608993.png)



接着我们stylecloud进行词云图，这样方便我们看看整体的分词效果如何，是否有一些词要不要去掉

最后的结果如下：

![词云图](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/%E8%AF%8D%E4%BA%91%E5%9B%BE.png)

把结果都处理好了就开始NLP，情感分析

## 2、情感分析

文本挖掘中，情感分析是经常需要使用到，而进行主题模型分析之前，对数据集进行文本分类再进行分析具有必要性，因为分类以后，每一类的主题才会更明显。而snownlp是一个python写的类库，可以方便的处理中文文本内容

 这里我们还是根据上面的方法，先进行分词处理，因为这样更有效帮助机器进行文本判断，从而给出正确的评价，这里有一点是要注意的

就是snownlp毕竟是广义的，无法做到精准判断该文本的正确评分，只能给出大概的数值，这也是所有机器学习的通病，毕竟是机器，NLP还是主观占比居多，所以提供的数值仅供参考

![image-20230207160140107](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20230207160140107.png)



接着我们获取到对应的分值之后，我们可以根据数据来做一个时间趋势图，从而得知，在每个月的一个分值走向，这里才去的是均值处理，把每个月的所有分值相加求平均值，所以这里还是有一定的参考价值，可以作为正确的评判标准，从这里我们可以得知，在2020-01 ： 2020-04,情绪都较为积极比较偏正向，这里分值是从0-1直接的，接近0则是负面，接近1则是正面

![情感趋势](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/%E6%83%85%E6%84%9F%E8%B6%8B%E5%8A%BF.png)



## 3、TF-IDF计算

![image-20230207160819153](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20230207160819153.png)

主要实现代码在这一块

```Python
    # 将文本中的词语转换为词频矩阵 矩阵元素a[i][j] 表示j词在i类文本下的词频
    vectorizer = CountVectorizer()

    # 该类会统计每个词语的tf-idf权值
    transformer = TfidfTransformer()

    # 第一个fit_transform是计算tf-idf 第二个fit_transform是将文本转为词频矩阵
    tfidf = transformer.fit_transform(vectorizer.fit_transform(corpus))
    # 获取词袋模型中的所有词语
    word = vectorizer.get_feature_names()

    # 将tf-idf矩阵抽取出来 元素w[i][j]表示j词在i类文本中的tf-idf权重
    weight = tfidf.toarray()

    data = {'word': word,
            'tfidf': weight.sum(axis=0).tolist()}
    df2 = pd.DataFrame(data)
    df2['tfidf'] = df2['tfidf'].astype('float64')
    df2 = df2.sort_values(by=['tfidf'], ascending=False)
```



我们从该文档中，抽取前20的词，会发现这里的词和高频词是一样的，但是它们的权重却又是不一样的，这就是高频词和TF-IDF之间的差异了

![TFIDF](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/TFIDF.png)

## 4、LDA主题建模

LDA参考文献：https://zhuanlan.zhihu.com/p/75222819

​			https://zhuanlan.zhihu.com/p/76636216

这里我们的步骤和上面和一样

首先我们还是先分词，把无效词给除去掉

接着我们开始构造主题数，寻找最优主题数，这里采用困惑度严格来说，判断标准并不合适，基于此我们这里采用的是另一种方式，也就是通过各个主题间的余弦相似度来衡量主题间的相似程度

![image-20230204223928349](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/202302042327880.png)

![image-20230204223941397](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/202302042327890.png)



具体代码实现方式

```python
# 构造主题数寻优函数
    def cos(vector1, vector2):  # 余弦相似度函数
        dot_product = 0.0
        normA = 0.0
        normB = 0.0
        for a, b in zip(vector1, vector2):
            dot_product += a * b
            normA += a ** 2
            normB += b ** 2
        if normA == 0.0 or normB == 0.0:
            return (None)
        else:
            return (dot_product / ((normA * normB) ** 0.5))

        # 主题数寻优

    def lda_k(x_corpus, x_dict):
        # 初始化平均余弦相似度
        mean_similarity = []
        mean_similarity.append(1)

        # 循环生成主题并计算主题间相似度
        for i in np.arange(2, 11):
            lda = models.LdaModel(x_corpus, num_topics=i, id2word=x_dict)  # LDA模型训练
            for j in np.arange(i):
                term = lda.show_topics(num_words=30)

            # 提取各主题词
            top_word = []
            for k in np.arange(i):

                top_word.append([''.join(re.findall('"(.*)"', i)) \
                                 for i in term[k][1].split('+')])  # 列出所有词

            # 构造词频向量
            word = sum(top_word, [])  # 列出所有的词
            unique_word = set(word)  # 去除重复的词

            # 构造主题词列表，行表示主题号，列表示各主题词
            mat = []
            for j in np.arange(i):
                top_w = top_word[j]
                mat.append(tuple([top_w.count(k) for k in unique_word]))

            p = list(itertools.permutations(list(np.arange(i)), 2))
            l = len(p)
            top_similarity = [0]
            for w in np.arange(l):
                vector1 = mat[p[w][0]]
                vector2 = mat[p[w][1]]
                top_similarity.append(cos(vector1, vector2))

            # 计算平均余弦相似度
            mean_similarity.append(sum(top_similarity) / l)
        return (mean_similarity)
```

处理好之后，再通过matplotlib来进行作图，在这里明显在5的时候处于谷底，因此根据这一点，我们可以选取主题数为5

![主题数寻优](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/%E4%B8%BB%E9%A2%98%E6%95%B0%E5%AF%BB%E4%BC%98.png)

从而进行建模，最后呈现的效果图如下：

可以通过点击不同的圆圈，来查看不同主题下，不同主题词的权重

![image-20230207161221502](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20230207161221502.png)