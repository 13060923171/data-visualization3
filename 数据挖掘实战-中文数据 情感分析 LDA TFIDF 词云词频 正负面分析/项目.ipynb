{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f3c16de-e275-426c-ab18-3b0a1cafd2a3",
   "metadata": {},
   "source": [
    "1、数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e80e57e-e478-4a09-8441-6d5b596c8261",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Administrator\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原数据总数: 5527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.397 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "清洗过后数据总数: 4836\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from snownlp import SnowNLP\n",
    "\n",
    "# 导入停用词列表\n",
    "stop_words = []\n",
    "with open(\"stopwords_cn.txt\", 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        stop_words.append(line.strip())\n",
    "\n",
    "\n",
    "#去掉标点符号，以及机械压缩\n",
    "def preprocess_word(word):\n",
    "    word1 = str(word)\n",
    "    word1 = re.sub(r'#\\w+#', '', word1)\n",
    "    word1 = re.sub(r'【.*?】', '', word1)\n",
    "    word1 = re.sub(r'@[\\w]+', '', word1)\n",
    "    word1 = re.sub(r'[a-zA-Z]', '', word1)\n",
    "    word1 = re.sub(r'\\.\\d+', '', word1)\n",
    "    return word1\n",
    "\n",
    "\n",
    "def emjio_tihuan(x):\n",
    "    x1 = str(x)\n",
    "    x2 = re.sub('(\\[.*?\\])', \"\", x1)\n",
    "    x3 = re.sub(r'@[\\w\\u2E80-\\u9FFF]+:?|\\[\\w+\\]', '', x2)\n",
    "    x4 = re.sub(r'\\n', '', x3)\n",
    "    return x4\n",
    "\n",
    "\n",
    "# 判断是否为中文\n",
    "def is_all_chinese(strs):\n",
    "    for _char in strs:\n",
    "        if not '\\u4e00' <= _char <= '\\u9fa5':\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_cut_words(content_series):\n",
    "    try:\n",
    "        # 对文本进行分词和词性标注\n",
    "        words = pseg.cut(content_series)\n",
    "        # 保存名词和形容词的列表\n",
    "        nouns_and_adjs = []\n",
    "        # 逐一检查每个词语的词性，并将名词和形容词保存到列表中\n",
    "        for word, flag in words:\n",
    "            #判断是否为名词或者形容词或者动词\n",
    "            if flag in ['Ag','a','ad','an','Ng','n','v']:\n",
    "                if word not in stop_words and len(word) >= 2 and is_all_chinese(word) == True:\n",
    "                    # 如果是名词或形容词，就将其保存到列表中\n",
    "                    nouns_and_adjs.append(word)\n",
    "        if len(nouns_and_adjs) != 0:\n",
    "            return ' '.join(nouns_and_adjs)\n",
    "        else:\n",
    "            return np.NAN\n",
    "    except:\n",
    "        return np.NAN\n",
    "\n",
    "\n",
    "def sentiment(x):\n",
    "    text = str(x)\n",
    "    s = SnowNLP(text)\n",
    "    sentiment = s.sentiments\n",
    "    if sentiment <= 0.3:\n",
    "        return \"负面\"\n",
    "    else:\n",
    "        return \"正面\"\n",
    "\n",
    "\n",
    "df = pd.read_excel('哔哩哔哩.xlsx')\n",
    "print('原数据总数:',len(df))\n",
    "df['内容'] = df['内容'].apply(preprocess_word)\n",
    "df['内容'] = df['内容'].apply(emjio_tihuan)\n",
    "df = df.dropna(subset=['内容'], axis=0)\n",
    "df['fenci'] = df['内容'].apply(get_cut_words)\n",
    "df = df.dropna(subset=['fenci'], axis=0)\n",
    "print('清洗过后数据总数:',len(df))\n",
    "df['sentiment'] = df['fenci'].apply(sentiment)\n",
    "df.to_csv('new_data.csv',index=False,encoding='utf-8-sig')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c26fd1-ef7f-44af-be65-96e9105f7c8a",
   "metadata": {},
   "source": [
    "2、情感分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b61dc3c-3273-479d-bba0-64495b7ef63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment\n",
      "负面    2591\n",
      "正面    2245\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.use('Agg')  # 使用Agg后端\n",
    "sns.set_style(style=\"whitegrid\")\n",
    "\n",
    "import random\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import imread\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "\n",
    "\n",
    "def emotion_pie():\n",
    "    df = pd.read_csv('new_data.csv')\n",
    "    new_df = df['sentiment'].value_counts()\n",
    "    print(new_df)\n",
    "    x_data = [x for x in new_df.index]\n",
    "    y_data = [y for y in new_df.values]\n",
    "    plt.figure(figsize=(9, 6), dpi=500)\n",
    "    plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "    plt.pie(y_data, labels=x_data, startangle=0, autopct='%1.2f%%')\n",
    "    plt.title('情感占比分布')\n",
    "    plt.tight_layout()\n",
    "    # 添加图例\n",
    "    plt.legend(x_data, loc='lower right')\n",
    "    plt.savefig('情感占比分布.png')\n",
    "\n",
    "\n",
    "def emotion_word(x):\n",
    "    df = pd.read_csv('new_data.csv')\n",
    "    df1 = df[df['sentiment'] == x]\n",
    "    d = {}\n",
    "    list_text = []\n",
    "    for t in df1['fenci']:\n",
    "        # 把数据分开\n",
    "        t = str(t).split(\" \")\n",
    "        for i in t:\n",
    "            list_text.append(i)\n",
    "            d[i] = d.get(i, 0) + 1\n",
    "\n",
    "    ls = list(d.items())\n",
    "    ls.sort(key=lambda x: x[1], reverse=True)\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    for key, values in ls[:100]:\n",
    "        x_data.append(key)\n",
    "        y_data.append(values)\n",
    "\n",
    "    data = pd.DataFrame()\n",
    "    data['word'] = x_data\n",
    "    data['counts'] = y_data\n",
    "\n",
    "    data.to_csv('{}-高频词Top100.csv'.format(x), encoding='utf-8-sig', index=False)\n",
    "\n",
    "    def color_func(word, font_size, position, orientation, random_state=None,\n",
    "                   **kwargs):\n",
    "        return \"hsl({}, 100%, 50%)\".format(np.random.randint(0, 300))\n",
    "\n",
    "    # 读取背景图片\n",
    "    background_Image = np.array(Image.open('image.jpg'))\n",
    "    text = ' '.join(list_text)\n",
    "    wc = WordCloud(\n",
    "        collocations=False,  # 禁用词组\n",
    "        font_path='simhei.ttf',  # 中文字体路径\n",
    "        margin=20,  # 词云图边缘宽度\n",
    "        mask=background_Image,  # 背景图形\n",
    "        scale=3,  # 放大倍数\n",
    "        max_words=200,  # 最多词个数\n",
    "        random_state=42,  # 随机状态\n",
    "        width=800,  # 图片宽度\n",
    "        height=600,  # 图片高度\n",
    "        min_font_size=15,  # 最小字体大小\n",
    "        max_font_size=90,  # 最大字体大小\n",
    "        background_color='#ecf0f1',  # 背景颜色\n",
    "        color_func=color_func  # 字体颜色函数\n",
    "    )\n",
    "    # 生成词云\n",
    "    wc.generate_from_text(text)\n",
    "    # 存储图像\n",
    "    wc.to_file(\"{}-top100-词云图.png\".format(x))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    emotion_pie()\n",
    "    list1 = ['正面','负面']\n",
    "    for l in list1:\n",
    "        emotion_word(l)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4265d3-e2ae-4500-bbc8-90d0be04edb9",
   "metadata": {},
   "source": [
    "3-4、TF-IDF评论文本关键词提取 正负面特征词和权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b87c118d-53bc-41ed-a29e-8657fd9addbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "def tf_idf(df,name):\n",
    "    corpus = []\n",
    "    for i in df['fenci']:\n",
    "        corpus.append(i.strip())\n",
    "\n",
    "        # 将文本中的词语转换为词频矩阵 矩阵元素a[i][j] 表示j词在i类文本下的词频\n",
    "    vectorizer = CountVectorizer()\n",
    "\n",
    "    # 该类会统计每个词语的tf-idf权值\n",
    "    transformer = TfidfTransformer()\n",
    "\n",
    "    # 第一个fit_transform是计算tf-idf 第二个fit_transform是将文本转为词频矩阵\n",
    "    tfidf = transformer.fit_transform(vectorizer.fit_transform(corpus))\n",
    "    # 获取词袋模型中的所有词语\n",
    "    word = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # 将tf-idf矩阵抽取出来 元素w[i][j]表示j词在i类文本中的tf-idf权重\n",
    "    weight = tfidf.toarray()\n",
    "\n",
    "    data = {'word': word,\n",
    "            'tfidf': weight.sum(axis=0).tolist()}\n",
    "\n",
    "    df2 = pd.DataFrame(data)\n",
    "    df2['tfidf'] = df2['tfidf'].astype('float64')\n",
    "    df2 = df2.sort_values(by=['tfidf'],ascending=False)\n",
    "    df2.to_csv('{}-TF-IDF相关数据.csv'.format(name),encoding='utf-8-sig',index=False)\n",
    "\n",
    "    df3 = df2.iloc[:30]\n",
    "    x_data = list(df3['word'])\n",
    "    y_data = list(df3['tfidf'])\n",
    "    x_data.reverse()\n",
    "    y_data.reverse()\n",
    "    plt.figure(figsize=(12, 9))\n",
    "    plt.barh(x_data, y_data)\n",
    "    plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "\n",
    "    plt.title(\"tf-idf 权重最高的top30词汇\")\n",
    "    plt.xlabel(\"权重\")\n",
    "    plt.savefig('{}-tf-idf top30.png'.format(name))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    list1 = ['正面','负面']\n",
    "    for l in list1:\n",
    "        df = pd.read_csv('new_data.csv')\n",
    "        df1 = df[df['sentiment'] == l]\n",
    "        tf_idf(df1,l)\n",
    "\n",
    "\n",
    "\n",
    "    df = pd.read_csv('new_data.csv')\n",
    "    tf_idf(df,'总数据')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7747b6fb-bcaf-44e3-80a2-0ef010fa08a2",
   "metadata": {},
   "source": [
    "5、LDA主题分析:主题数确定、主题提取、主题及其对应特征词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f858df2-ebb0-40af-b98e-9b0ae454ca67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:137: DeprecationWarning: invalid escape sequence '\\*'\n",
      "<>:142: DeprecationWarning: invalid escape sequence '\\d'\n",
      "<>:137: DeprecationWarning: invalid escape sequence '\\*'\n",
      "<>:142: DeprecationWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9288\\1666936228.py:137: DeprecationWarning: invalid escape sequence '\\*'\n",
      "  c1 = re.compile('\\*\"(.*?)\"')\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9288\\1666936228.py:142: DeprecationWarning: invalid escape sequence '\\d'\n",
      "  c4 = re.compile(\".*?(\\d+).*?\")\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 14/14 [01:48<00:00,  7.75s/it]\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9288\\1666936228.py:116: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['主题概率'] = list3\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9288\\1666936228.py:117: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['主题类型'] = list2\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 14/14 [01:51<00:00,  7.95s/it]\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9288\\1666936228.py:116: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['主题概率'] = list3\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9288\\1666936228.py:117: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['主题类型'] = list2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# 数据处理库\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.use('Agg')  # 使用Agg后端\n",
    "\n",
    "import re\n",
    "import os\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "\n",
    "#LDA建模\n",
    "def lda(df,emotion):\n",
    "    train = []\n",
    "    stop_word = []\n",
    "    with open(\"stopwords_cn.txt\", 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            stop_word.append(line.strip())\n",
    "    for line in df['fenci']:\n",
    "        line = [str(word).strip(' ') for word in line.split(' ') if len(word) >= 2 and word not in stop_word]\n",
    "        train.append(line)\n",
    "\n",
    "    #构建为字典的格式\n",
    "    dictionary = corpora.Dictionary(train)\n",
    "    corpus = [dictionary.doc2bow(text) for text in train]\n",
    "    \n",
    "    if not os.path.exists(\"./{}\".format(emotion)):\n",
    "        os.mkdir(\"./{}\".format(emotion))\n",
    "\n",
    "\n",
    "    # 困惑度模块\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    z_data = []\n",
    "    for i in tqdm(range(2, 16)):\n",
    "        x_data.append(i)\n",
    "        lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,id2word=dictionary,num_topics=i)\n",
    "        # 困惑度计算\n",
    "        perplexity = lda_model.log_perplexity(corpus)\n",
    "        y_data.append(perplexity)\n",
    "        # 一致性计算\n",
    "        coherence_model_lda = CoherenceModel(model=lda_model, texts=train, dictionary=dictionary, coherence='c_v')\n",
    "        coherence = coherence_model_lda.get_coherence()\n",
    "        z_data.append(coherence)\n",
    "\n",
    "    # 绘制困惑度和一致性折线图\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "    matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "    # 绘制困惑度折线图\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    plt.plot(x_data, y_data, marker=\"o\")\n",
    "    plt.title(\"perplexity_values\")\n",
    "    plt.xlabel('num topics')\n",
    "    plt.ylabel('perplexity score')\n",
    "    #绘制一致性的折线图\n",
    "    ax2 = fig.add_subplot(1, 2, 2)\n",
    "    plt.plot(x_data, z_data, marker=\"o\")\n",
    "    plt.title(\"coherence_values\")\n",
    "    plt.xlabel(\"num topics\")\n",
    "    plt.ylabel(\"coherence score\")\n",
    "\n",
    "    plt.savefig('./{}/困惑度和一致性.png'.format(emotion))\n",
    "\n",
    "    #将上面获取的数据进行保存\n",
    "    df5 = pd.DataFrame()\n",
    "    df5['主题数'] = x_data\n",
    "    df5['困惑度'] = y_data\n",
    "    df5['一致性'] = z_data\n",
    "    df5.to_csv('./{}/困惑度和一致性.csv'.format(emotion),encoding='utf-8-sig',index=False)\n",
    "\n",
    "    optimal_z = max(z_data)\n",
    "    optimal_z_index = z_data.index(optimal_z)\n",
    "    best_topic_number = x_data[optimal_z_index]\n",
    "    num_topics = best_topic_number\n",
    "    #LDA可视化模块\n",
    "    #构建lda主题参数\n",
    "    lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=111, iterations=400)\n",
    "    #读取lda对应的数据\n",
    "    data1 = pyLDAvis.gensim.prepare(lda, corpus, dictionary)\n",
    "    #把数据进行可视化处理\n",
    "    pyLDAvis.save_html(data1, './{}/lda.html'.format(emotion))\n",
    "\n",
    "    #主题判断模块\n",
    "    list3 = []\n",
    "    list2 = []\n",
    "    #这里进行lda主题判断\n",
    "    for i in lda.get_document_topics(corpus)[:]:\n",
    "        listj = []\n",
    "        list1 = []\n",
    "        for j in i:\n",
    "            list1.append(j)\n",
    "            listj.append(j[1])\n",
    "        list3.append(list1)\n",
    "        bz = listj.index(max(listj))\n",
    "        list2.append(i[bz][0])\n",
    "\n",
    "\n",
    "    df['主题概率'] = list3\n",
    "    df['主题类型'] = list2\n",
    "\n",
    "    df.to_csv('./{}/lda_data.csv'.format(emotion),encoding='utf-8-sig',index=False)\n",
    "\n",
    "    data = df\n",
    "    #获取对应主题出现的频次\n",
    "    new_data = data['主题类型'].value_counts()\n",
    "    new_data = new_data.sort_index(ascending=True)\n",
    "    y_data1 = [y for y in new_data.values]\n",
    "\n",
    "    #主题词模块\n",
    "    word = lda.print_topics(num_words=20)\n",
    "    topic = []\n",
    "    quanzhong = []\n",
    "    list_gailv = []\n",
    "    list_gailv1 = []\n",
    "    list_word = []\n",
    "    #根据其对应的词，来获取其相应的权重\n",
    "    for w in word:\n",
    "        ci = str(w[1])\n",
    "        c1 = re.compile('\\*\"(.*?)\"')\n",
    "        c2 = c1.findall(ci)\n",
    "        list_word.append(c2)\n",
    "        c3 = '、'.join(c2)\n",
    "\n",
    "        c4 = re.compile(\".*?(\\d+).*?\")\n",
    "        c5 = c4.findall(ci)\n",
    "        for c in c5[::1]:\n",
    "            if c != \"0\":\n",
    "                gailv = str(0) + '.' + str(c)\n",
    "                list_gailv.append(gailv)\n",
    "        list_gailv1.append(list_gailv)\n",
    "        list_gailv = []\n",
    "        zt = \"Topic\" + str(w[0])\n",
    "        topic.append(zt)\n",
    "        quanzhong.append(c3)\n",
    "\n",
    "    #把上面权重的词计算好之后，进行保存为csv文件\n",
    "    df2 = pd.DataFrame()\n",
    "    for j,k,l in zip(topic,list_gailv1,list_word):\n",
    "        df2['{}-主题词'.format(j)] = l\n",
    "        df2['{}-权重'.format(j)] = k\n",
    "    df2.to_csv('./{}/主题词分布表.csv'.format(emotion), encoding='utf-8-sig', index=False)\n",
    "\n",
    "    y_data2 = []\n",
    "    for y in y_data1:\n",
    "        number = float(y / sum(y_data1))\n",
    "        y_data2.append(float('{:0.5}'.format(number)))\n",
    "\n",
    "    df1 = pd.DataFrame()\n",
    "    df1['所属主题'] = topic\n",
    "    df1['文章数量'] = y_data1\n",
    "    df1['特征词'] = quanzhong\n",
    "    df1['主题强度'] = y_data2\n",
    "    df1.to_csv('./{}/特征词.csv'.format(emotion),encoding='utf-8-sig',index=False)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    list1 = ['正面','负面']\n",
    "    for l in list1:\n",
    "        df = pd.read_csv('new_data.csv')\n",
    "        df1 = df[df['sentiment'] == l]\n",
    "        lda(df1,l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3150f699-1829-4c46-a9f6-526b3db999e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
